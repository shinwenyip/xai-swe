{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import project\n",
    "from project import Project \n",
    "import functions\n",
    "from functions import *\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can reload imported lib without restarting kernel after updating lib\n",
    "importlib.reload(functions)\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(project)\n",
    "from project import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_names = ['ANT','CAMEL','JEDIT']\n",
    "models = ['LR', 'SVM','KNN', 'NB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare project and model for all \n",
    "projects = {}\n",
    "\n",
    "for project_name in project_names:\n",
    "    proj = Project(project_name)\n",
    "    print(proj.name)\n",
    "    proj.set_train_test()\n",
    "    projects[project_name] = proj\n",
    "    for model in models:\n",
    "        proj.train_global_model(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 1 : local prediction fidelity\n",
    "# get predictions for global, lime, pyexplainer, shap\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for proj in projects:\n",
    "    curr_project = projects[proj]\n",
    "    X_train = curr_project.X_train\n",
    "    y_train = curr_project.y_train\n",
    "\n",
    "    for model in models:\n",
    "        global_model = curr_project.models[model]\n",
    "        \n",
    "        limeExp,lime_explanations = get_explanations(curr_project, 'lime',model,X_train,y_train,global_model )\n",
    "        pyExp,pyexplanations = get_explanations(curr_project, 'pyExp',model,X_train,y_train,global_model )\n",
    "        shapExp,shap_explanations = get_explanations(curr_project, 'shap',model,X_train,y_train,global_model )\n",
    "\n",
    "        lime_preds = [list(exp['rule'].local_pred.values())[0][0] for exp in lime_explanations] #lime local prediction probabilities\n",
    "        global_preds = [exp['rule'].predict_proba[1] for exp in lime_explanations] #global pred probas\n",
    "        py_preds = [exp['local_model'].predict_proba(exp['X_explain'].values)[0][1] for exp in pyexplanations]\n",
    "        shap_preds = [shapExp.expected_value + sum(exp['shap_values']) for exp in shap_explanations]\n",
    "        \n",
    "        model_df = pd.DataFrame(prediction_fidelity(global_preds,lime_preds,py_preds,shap_preds))\n",
    "        model_df['model'] = model\n",
    "        model_df['project'] = proj\n",
    "        df = pd.concat([df,model_df])\n",
    "\n",
    "print(df)\n",
    "df.to_csv('eval_results/metric1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 2 : internal fidelity \n",
    "model = 'LR'\n",
    "df = pd.DataFrame()\n",
    "for proj in projects:\n",
    "    curr_project = projects[proj]\n",
    "    X_test = curr_project.X_test\n",
    "    y_test = curr_project.y_test\n",
    "    X_train = curr_project.X_train\n",
    "    y_train = curr_project.y_train\n",
    "\n",
    "    global_model = curr_project.models[model] \n",
    "\n",
    "    test_data_x,test_data_y,_ = curr_project.get_sampled_data(model)\n",
    "    \n",
    "    limeExp,lime_explanations = get_explanations(curr_project, 'lime',model,X_train,y_train,global_model )\n",
    "    pyExp,pyexplanations = get_explanations(curr_project, 'pyExp',model,X_train,y_train,global_model )\n",
    "    shapExp,shap_explanations = get_explanations(curr_project, 'shap',model,X_train,y_train,global_model )\n",
    "\n",
    "    model_df = pd.DataFrame(internal_fidelity(global_model,test_data_x,test_data_y,lime_explanations,pyexplanations,shap_explanations))  \n",
    "    recalls = model_df['recalls']\n",
    "    model_df = model_df.iloc[:,:-1]\n",
    "    model_df['model'] = model\n",
    "    model_df['project'] = proj\n",
    "    df = pd.concat([df,model_df])\n",
    "print(df)\n",
    "df.to_csv('eval_results/metric2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 3 : faithfulness\n",
    "df = pd.DataFrame()\n",
    "fs = pd.DataFrame()\n",
    "\n",
    "for proj in projects:\n",
    "    curr_project = projects[proj]\n",
    "    X_train = curr_project.X_train\n",
    "    y_train = curr_project.y_train\n",
    "    \n",
    "\n",
    "    for model in models:\n",
    "        test_data_x,test_data_y,_ = curr_project.get_sampled_data(model)\n",
    "        global_model = curr_project.models[model]\n",
    "        \n",
    "        limeExp,lime_explanations = get_explanations(curr_project, 'lime',model,X_train,y_train,global_model )\n",
    "        pyExp,pyexplanations = get_explanations(curr_project, 'pyExp',model,X_train,y_train,global_model )\n",
    "        shapExp,shap_explanations = get_explanations(curr_project, 'shap',model,X_train,y_train,global_model )\n",
    "\n",
    "        model_df = pd.DataFrame(faithfulness(global_model, test_data_x, lime_explanations, pyexplanations, shap_explanations))\n",
    "        for i in range(3):\n",
    "            testdf = pd.DataFrame()\n",
    "            score = model_df.iloc[i,2]\n",
    "            testdf['faithfulness_score'] = score\n",
    "            testdf['method'] = model_df.iloc[i,0]\n",
    "            testdf['model'] = model\n",
    "            testdf['project'] = proj\n",
    "            fs = pd.concat([fs ,testdf])\n",
    "        \n",
    "        faithfulness_scores = model_df[['faithfulness_scores','method']]\n",
    "        # model_df = model_df.iloc[:,:-1]\n",
    "        model_df['model'] = model\n",
    "        model_df['project'] = proj\n",
    "        df = pd.concat([df,model_df])\n",
    "\n",
    "print(df)\n",
    "df.to_csv('eval_results/metric3.csv')\n",
    "fs.to_csv('eval_results/fs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 4 : monotonicity\n",
    "df = pd.DataFrame()\n",
    "for proj in projects:\n",
    "    curr_project = projects[proj]\n",
    "    X_train = curr_project.X_train\n",
    "    y_train = curr_project.y_train\n",
    "    \n",
    "\n",
    "    for model in models:\n",
    "        global_model = curr_project.models[model]\n",
    "        test_data_x,test_data_y,_ = curr_project.get_sampled_data(model)\n",
    "\n",
    "        limeExp,lime_explanations = get_explanations(curr_project, 'lime',model,X_train,y_train,global_model )\n",
    "        pyExp,pyexplanations = get_explanations(curr_project, 'pyExp',model,X_train,y_train,global_model )\n",
    "        shapExp,shap_explanations = get_explanations(curr_project, 'shap',model,X_train,y_train,global_model )\n",
    "\n",
    "        model_df = pd.DataFrame(monotonicity(global_model, test_data_x, lime_explanations, pyexplanations, shap_explanations))\n",
    "        monotonicity_scores = model_df['monotonicity_scores']\n",
    "        model_df = model_df.iloc[:,:-1]\n",
    "        model_df['model'] = model\n",
    "        model_df['project'] = proj\n",
    "        df = pd.concat([df,model_df])\n",
    "\n",
    "print(df)\n",
    "df.to_csv('eval_results/metric4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 5 : uniqueness\n",
    "df = pd.DataFrame()\n",
    "for proj in projects:\n",
    "    curr_project = projects[proj]\n",
    "    X_train = curr_project.X_train\n",
    "    y_train = curr_project.y_train\n",
    "    \n",
    "\n",
    "    for model in models:\n",
    "        global_model = curr_project.models[model]\n",
    "        test_data_x,test_data_y,_ = curr_project.get_sampled_data(model)\n",
    "        limeExp,lime_explanations = get_explanations(curr_project, 'lime',model,X_train,y_train,global_model )\n",
    "        pyExp,pyexplanations = get_explanations(curr_project, 'pyExp',model,X_train,y_train,global_model )\n",
    "        shapExp,shap_explanations = get_explanations(curr_project, 'shap',model,X_train,y_train,global_model )\n",
    "\n",
    "        model_df = pd.DataFrame(uniqueness(global_model,test_data_x, lime_explanations, pyexplanations, shap_explanations))\n",
    "        uniqueness_scores = model_df['uniqueness']\n",
    "        model_df['model'] = model\n",
    "        model_df['project'] = proj\n",
    "        df = pd.concat([df,model_df])\n",
    "\n",
    "print(df)\n",
    "df.to_csv('eval_results/metric5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 6 : similarity\n",
    "df = pd.DataFrame()\n",
    "for proj in projects:\n",
    "    curr_project = projects[proj]\n",
    "    X_train = curr_project.X_train\n",
    "    y_train = curr_project.y_train\n",
    "\n",
    "\n",
    "    for model in models:\n",
    "        global_model = curr_project.models[model]\n",
    "        test_data_x,test_data_y,_ = curr_project.get_sampled_data(model)\n",
    "\n",
    "        limeExp,lime_explanations = get_explanations(curr_project, 'lime',model,X_train,y_train,global_model )\n",
    "        pyExp,pyexplanations = get_explanations(curr_project, 'pyExp',model,X_train,y_train,global_model )\n",
    "\n",
    "        model_df = pd.DataFrame(similarity(test_data_x, lime_explanations, pyexplanations))\n",
    "        euc_dist_med = model_df['euc_dist_med']\n",
    "        model_df['model'] = model\n",
    "        model_df['project'] = proj\n",
    "        df = pd.concat([df,model_df])\n",
    "\n",
    "print(df)\n",
    "df.to_csv('eval_results/metric6.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
