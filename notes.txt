1. get train test data
2. Prepare data (project)
    - Oversample minority class (defective class) - training data
    - Binarize or Standardise data - features (X train and x test)
    - Return (X_train_bin, X_test_bin, X_trainStd, X_testStd, featureBinarizer)
4. Train model(project,model_name)
    - X_train_bin, X_test_bin, X_trainStd, X_testStd, featureBinarizer = prepare_data(project)


Project object
- project_name
- X_train , X_test , y_train, y_test
- X_train_rs, y_train_rs
- featureBinarizer
- X_train_bin, X_test_bin
- X_trainStd, X_testStd
- models []: SVM, LR, BRCG, LogRR
- explainer_dir = os.path.join('./explainer/',project_name)
- model_dir

1. define project_name
2. Create ant = project object(project_name)
3. (in project) define get_datasets() : if project_name = .... -> train_data, test_data
4. 

for BRCG  fit x_train_bin, y_train_rs       and X_test_bin
LogRR fit x_train_bin, y_train_rs, X_trainStd
others: fit self.X_trainStd,self.y_train_rs

predicting
LogRR: pred = self.model.predict(self.X_test_bin, self.X_testStd)
        defect_prob = self.model.predict_proba(self.X_test_bin, self.X_testStd)
BRCG: pred = self.model.predict(self.X_test_bin)
others: pred = self.model.predict(self.X_testStd)
            defect_prob = self.model.predict_proba(self.X_test)[:,1]
if defect_prob: return pred, defect_prob else return pred

how each model explains
model = ant.models[model_name]
    
pyexp:
'synthetic_data', 'synthetic_predictions', 'X_explain', 'y_explain', 'indep', 'dep', 'top_k_positive_rules', 'top_k_negative_rule,


todo:
explanation for CEM, LIME, SHAP, PyExplainer
metrics compare for CEM,LIME,SHAP,PyExplainer - using IBM metrics + metrics we used

qn:
how to find base values for lime explanation

index   rule  type      coef support  importance  is_satisfy_instance  

explainers: [SHAP, LIME, PyExplainer, CEM]
models = [LR, SVM, NB]
1. train data on LR, SVM, KNN, NB
2. for each trained models, generate explanations

commands:
conda env export --no-builds --from-history -f env.yml

ResolvePackageNotFound: 
  - m2w64-gcc-libs=5.3.0
  - wincertstore=0.2
  - winpty=0.4.3
  - vc=14.2
  - intel-openmp=2022.0.0
  - m2w64-gcc-libs-core=5.3.0
  - icc_rt=2019.0.0
  - msys2-conda-epoch=20160418
  - vs2015_runtime=14.29.30037
  - ucrt=10.0.20348.0
  - m2w64-gcc-libgfortran=5.3.0
  - m2w64-libwinpthread-git=5.0.0.4634.697f757
  - m2w64-gmp=6.1.0
  - pywinpty=1.1.6


PyExplainer explanation
explanation = pyExp.explain(X_explain,y_explain, search_function='crossoverinterpolation',random_state=0,reuse_local_model=True)
# returns a dictionary with keys: 
- dict_keys(['synthetic_data', 'synthetic_predictions', 'X_explain', 'y_explain', 'indep', 'dep', 'top_k_positive_rules', 'top_k_negative_rules', 'local_rulefit_model'])
X_explain is the features of the instance of interest
y_explain is the prediction of the instance of interest
indep is the list of independent variable names (feature names)
dep is the dependent variable name (name of prediction column)

for uniqueness metric in pyExplainer paper, 
for each instance of interest:
  method extracts the top 1 rule for lime and pyexplanation (for pyexplanation top rule is split into multiple propositions)
  uniqueness is measured by: 
    total unique rules / total rules 
  # total rules would be more for pyexplainer as each instance explanation have multiple propositions
if we use this metric on shap, 
  do we extract the top (feature,importance) pair and find percentage uniqueness?





Experiment notes:
1. SHAP shap_values(self, X, **kwargs) parameter - l1_reg : "num_features(int)" # set to 10
2. save Explainer data also
3. not sure why camel knn lime predicting all negative... < 0.5