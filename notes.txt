1. get train test data
2. Prepare data (project)
    - Oversample minority class (defective class) - training data
    - Binarize or Standardise data - features (X train and x test)
    - Return (X_train_bin, X_test_bin, X_trainStd, X_testStd, featureBinarizer)
4. Train model(project,model_name)
    - X_train_bin, X_test_bin, X_trainStd, X_testStd, featureBinarizer = prepare_data(project)


Project object
- project_name
- X_train , X_test , y_train, y_test
- X_train_rs, y_train_rs
- featureBinarizer
- X_train_bin, X_test_bin
- X_trainStd, X_testStd
- models []: SVM, LR, BRCG, LogRR
- explainer_dir = os.path.join('./explainer/',project_name)
- model_dir

1. define project_name
2. Create ant = project object(project_name)
3. (in project) define get_datasets() : if project_name = .... -> train_data, test_data
4. 

for BRCG  fit x_train_bin, y_train_rs       and X_test_bin
LogRR fit x_train_bin, y_train_rs, X_trainStd
others: fit self.X_trainStd,self.y_train_rs

predicting
LogRR: pred = self.model.predict(self.X_test_bin, self.X_testStd)
        defect_prob = self.model.predict_proba(self.X_test_bin, self.X_testStd)
BRCG: pred = self.model.predict(self.X_test_bin)
others: pred = self.model.predict(self.X_testStd)
            defect_prob = self.model.predict_proba(self.X_test)[:,1]
if defect_prob: return pred, defect_prob else return pred

how each model explains
model = ant.models[model_name]
    
pyexp:
'synthetic_data', 'synthetic_predictions', 'X_explain', 'y_explain', 'indep', 'dep', 'top_k_positive_rules', 'top_k_negative_rule,


todo:
explanation for CEM, LIME, SHAP, PyExplainer
metrics compare for CEM,LIME,SHAP,PyExplainer - using IBM metrics + metrics we used

qn:
how to find base values for lime explanation

index   rule  type      coef support  importance  is_satisfy_instance  

explainers: [SHAP, LIME, PyExplainer, CEM]
models = [LR, SVM, NB]
1. train data on LR, SVM, KNN, NB
2. for each trained models, generate explanations

commands:
conda env export --no-builds --from-history -f env.yml

ResolvePackageNotFound: 
  - m2w64-gcc-libs=5.3.0
  - wincertstore=0.2
  - winpty=0.4.3
  - vc=14.2
  - intel-openmp=2022.0.0
  - m2w64-gcc-libs-core=5.3.0
  - icc_rt=2019.0.0
  - msys2-conda-epoch=20160418
  - vs2015_runtime=14.29.30037
  - ucrt=10.0.20348.0
  - m2w64-gcc-libgfortran=5.3.0
  - m2w64-libwinpthread-git=5.0.0.4634.697f757
  - m2w64-gmp=6.1.0
  - pywinpty=1.1.6


PyExplainer explanation
explanation = pyExp.explain(X_explain,y_explain, search_function='crossoverinterpolation',random_state=0,reuse_local_model=True)
# returns a dictionary with keys: 
- dict_keys(['synthetic_data', 'synthetic_predictions', 'X_explain', 'y_explain', 'indep', 'dep', 'top_k_positive_rules', 'top_k_negative_rules', 'local_rulefit_model'])
X_explain is the features of the instance of interest
y_explain is the prediction of the instance of interest
indep is the list of independent variable names (feature names)
dep is the dependent variable name (name of prediction column)

for uniqueness metric in pyExplainer paper, 
for each instance of interest:
  method extracts the top 1 rule for lime and pyexplanation (for pyexplanation top rule is split into multiple propositions)
  uniqueness is measured by: 
    total unique rules / total rules 
  # total rules would be more for pyexplainer as each instance explanation have multiple propositions
if we use this metric on shap, 
  do we extract the top (feature,importance) pair and find percentage uniqueness?


        method  total_monotonicity model project
0         LIME                   0    LR     ANT
1         SHAP                   1    LR     ANT
2  pyExplainer                   0    LR     ANT
0         LIME                   0   SVM     ANT
1         SHAP                  57   SVM     ANT
2  pyExplainer                  19   SVM     ANT
0         LIME                  66   KNN     ANT
1         SHAP                  80   KNN     ANT
2  pyExplainer                   5   KNN     ANT
0         LIME                   2    LR   CAMEL
1         SHAP                   1    LR   CAMEL
2  pyExplainer                   0    LR   CAMEL
0         LIME                   0   SVM   CAMEL
1         SHAP                   6   SVM   CAMEL
2  pyExplainer                   0   SVM   CAMEL
0         LIME                  25   KNN   CAMEL
1         SHAP                  33   KNN   CAMEL
2  pyExplainer                   2   KNN   CAMEL


Experiment notes:
1. SHAP shap_values(self, X, **kwargs) parameter - l1_reg : "num_features(int)" # set to 10
2. save Explainer data also
3. not sure why camel knn lime predicting all negative... < 0.5
4. increase iteration for LR for convergence -done
5. performance for models are quite bad..
6. knn faithfulness predictions all the same..
7. check shap logit link 
TODO - write in detail for each metric how it works

Dataset project: ANT
SVM Training accuracy(in %): 76.83397683397683
SVM Test accuracy(in %): 76.51006711409396
SVM precision(in %): 48.08510638297872
SVM recall(in %): 68.07228915662651
SVM f1(in %): 56.35910224438903